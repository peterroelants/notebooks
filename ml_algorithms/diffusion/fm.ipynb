{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Minimal PyTorch implementation of Flow Matching with Optimal Transport (OT) for generative modeling, based on \"Flow Matching for Generative Modeling\" by Lipman et al. (2023).\n",
    "\n",
    "This code provides a simplified example using the half-moons dataset and Euler method for sampling.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import make_moons\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple\n",
    "\n",
    "# Device configuration\n",
    "device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "data_size: int = 10_000\n",
    "batch_size: int = 256\n",
    "input_dim: int = 2\n",
    "hidden_dim: int = 64\n",
    "n_iterations: int = 10_000  # Total number of iterations.\n",
    "sigma_min: float = (\n",
    "    1e-5  # For OT path (Eq. 22). Controls variance at t=1 (should be small).\n",
    ")\n",
    "lr: float = 1e-3\n",
    "\n",
    "# Data #######################################################################\n",
    "# Generate half moon data as toy dataset\n",
    "data, _ = make_moons(data_size, noise=0.05)\n",
    "data = torch.tensor(data, dtype=torch.float32).to(device)\n",
    "data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# Model ######################################################################\n",
    "# Define the vector field network v_t\n",
    "class VectorFieldNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Parametric model for the time-dependent vector field v_t(x; θ) (Section 3 in Lipman et al.).\n",
    "\n",
    "    This network represents the function v_t(x; θ), which is the Continuous Normalizing Flow (CNF) vector field.\n",
    "    It is used in the Flow Matching objective (Eq. 5 in Lipman et al.).\n",
    "    The network outputs the *value* of the vector field at a given time t and point x.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        # MLP architecture (no specific architecture defined in the paper)\n",
    "        self.net: nn.Sequential = nn.Sequential(\n",
    "            nn.Linear(input_dim + 1, hidden_dim),  # +1 for time embedding\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "        )\n",
    "\n",
    "    def time_embedding(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Embeds time into a higher dimensional space.\n",
    "        \"\"\"\n",
    "        return t.unsqueeze(-1)  # Simple embedding, no frequencies used\n",
    "\n",
    "    def forward(self, t: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the vector field v_t(x) at time t and point x.\n",
    "        \"\"\"\n",
    "        t_emb: torch.Tensor = self.time_embedding(t)\n",
    "        tx: torch.Tensor = torch.cat([t_emb, x], dim=-1)\n",
    "        return self.net(tx)  # Output is v_t(x; θ) (Eq. 5 in Lipman et al.)\n",
    "\n",
    "\n",
    "def ot_path(\n",
    "    x_0: torch.Tensor,\n",
    "    x_1: torch.Tensor,\n",
    "    t: torch.Tensor,\n",
    "    sigma_min: float,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Computes the Optimal Transport (OT) path (ψ_t) and target vector field (u_t).\n",
    "\n",
    "    This function implements the conditional flow ψ_t (Eq. 22 in Lipman et al.) and the corresponding target vector field u_t (Eq. 23 in Lipman et al.) for the Optimal Transport (OT) formulation in Flow Matching.  ψ_t defines a straight-line interpolation path between a sample x_0 from the prior distribution and a target data point x_1.  The target vector field u_t represents the ideal vector field that the learned vector field v_t should approximate.\n",
    "\n",
    "    Args:\n",
    "        x_0: Samples from the prior distribution (typically a standard Gaussian).\n",
    "        x_1: Target data points.\n",
    "        t: Time points along the OT path (between 0 and 1).\n",
    "        sigma_min: Small constant controlling the variance at t=1 (see Eq. 22).\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the OT path (ψ_t) and the target vector field (u_t).\n",
    "    \"\"\"\n",
    "    t = t.unsqueeze(-1)\n",
    "    # ψ_t (Eq. 22 in Lipman et al.) Linear interpolation between prior and target.\n",
    "    psi_t: torch.Tensor = (1 - (1 - sigma_min) * t) * x_0 + (t * x_1)\n",
    "    # Target vector field for OT (u_t) (Eq. 23 in Lipman et al.)\n",
    "    target_v: torch.Tensor = x_1 - (1 - sigma_min) * x_0\n",
    "    return psi_t, target_v\n",
    "\n",
    "\n",
    "def compute_cfm_loss(\n",
    "    v_net: VectorFieldNetwork,\n",
    "    x_0: torch.Tensor,\n",
    "    x_1: torch.Tensor,\n",
    "    t: torch.Tensor,\n",
    "    sigma_min: float,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the Conditional Flow Matching (CFM) loss for Optimal Transport.\n",
    "\n",
    "    This function computes the CFM loss (Eq. 23 in Lipman et al.) using the Optimal Transport (OT) formulation.  It compares the predicted vector field v_t(ψ_t(x_0, x_1, t)) (output of the v_net) with the target vector field u_t derived from the OT path. The loss is calculated as the mean squared error (MSE) between the predicted and target vector fields.  Minimizing this loss encourages the learned vector field to accurately represent the OT flow.\n",
    "\n",
    "    Args:\n",
    "        v_net: The vector field network v_t(x; θ).\n",
    "        x_0: Samples from the prior distribution.\n",
    "        x_1: Target data points.\n",
    "        t: Time points along the OT path.\n",
    "        sigma_min: Small constant controlling the variance at t=1.\n",
    "\n",
    "    Returns:\n",
    "        The CFM loss as a scalar tensor.\n",
    "    \"\"\"\n",
    "    psi_t, target_v = ot_path(x_0, x_1, t, sigma_min)\n",
    "    v: torch.Tensor = v_net(t, psi_t)  # v_t(ψ_t(x_0, x_1, t))\n",
    "    # CFM loss (Eq. 23 in Lipman et al.), simplified as MSE loss\n",
    "    loss: torch.Tensor = ((v - target_v) ** 2).mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Training ###################################################################\n",
    "# Initialize the vector field network v_t and optimizer\n",
    "v_net = VectorFieldNetwork(input_dim, hidden_dim).to(device)\n",
    "optimizer = optim.Adam(v_net.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "progress_bar = tqdm(range(n_iterations), desc=\"Training\", unit=\"iteration\")\n",
    "losses = []  # List to store loss values\n",
    "for iteration in progress_bar:\n",
    "    x_1 = next(iter(data_loader))  # Sample data x_1 ~ q(x_1)\n",
    "    t = torch.rand(x_1.shape[0], device=device)  # Sample time t ~ U[0, 1]\n",
    "    x_0 = torch.randn_like(x_1)  # Sample prior noise x_0 ~ N(0, I)\n",
    "\n",
    "    loss = compute_cfm_loss(v_net, x_0, x_1, t, sigma_min)  # CFM loss\n",
    "\n",
    "    # Optimization step\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    losses.append(loss.item())  # Store the loss value\n",
    "    progress_bar.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "# Plot loss curve after training\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.plot(losses)\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_title(\"Training Loss Curve\")\n",
    "ax.grid(True)  # Add grid\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Sampling ###################################################################\n",
    "def sample(\n",
    "    n_samples: int,\n",
    "    vector_field: VectorFieldNetwork,\n",
    "    dt: float = 0.01,  # Step size for Euler integration\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generates samples by integrating the learned vector field.\n",
    "\n",
    "    This function generates samples by numerically integrating the learned vector field `v_t(x)` from t=0 to t=1.  It uses the Euler method (a simple first-order numerical integration method) to approximate the solution to the ODE defined by the vector field (Eq. 1 in Lipman et al. with a simplified discretization).  The integration starts from random samples drawn from the prior distribution (typically a standard Gaussian).\n",
    "\n",
    "    Args:\n",
    "        n_samples: The number of samples to generate.\n",
    "        vector_field: The learned vector field network v_t(x; θ).\n",
    "        dt: The integration step size.\n",
    "\n",
    "    Returns:\n",
    "        A tensor of generated samples.\n",
    "    \"\"\"\n",
    "    x = torch.randn(n_samples, input_dim).to(device)  # Sample x_0 ~ N(0, I)\n",
    "    for t in torch.arange(0, 1, dt):  # Euler integration from t=0 to t=1\n",
    "        t_batch = t.expand(n_samples).to(device)\n",
    "        x = (\n",
    "            x + vector_field(t_batch, x) * dt\n",
    "        )  # Euler update (Eq. 28 in Lipman et al. with simplified discretization)\n",
    "    return x\n",
    "\n",
    "\n",
    "# Sample and plot generated data\n",
    "generated_samples = sample(data_size, v_net)\n",
    "generated_samples = generated_samples.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "# Visualization ###############################################################\n",
    "# Plot the generated samples\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.scatter(\n",
    "    data[:, 0].cpu().numpy(), data[:, 1].cpu().numpy(), label=\"Real Data\", alpha=0.1\n",
    ")\n",
    "ax.scatter(\n",
    "    generated_samples[:, 0], generated_samples[:, 1], label=\"Generated Data\", alpha=0.1\n",
    ")\n",
    "ax.set_title(\"Flow Matching with Optimal Transport - Generated Samples\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
