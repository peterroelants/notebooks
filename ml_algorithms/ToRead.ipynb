{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## GANS\n",
    "\n",
    "\"The generative model can be thought of as analogous to a team of counterfeiters, trying to produce fake currency and use it without detection, while the discriminative model is analogous to the police, trying to detect the counterfeit currency.\"\n",
    "\"In the space of arbitraryfunctions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere.\"\n",
    "\"The generator G implicitly defines a probability distribution pg as the distribution of the samples G(z) obtained when z ∼ pz.\" - Generative Adversarial Nets paper: https://arxiv.org/pdf/1406.2661.pdf\n",
    "\n",
    "\n",
    "- \n",
    "[Generalization and Equilibrium in Generative Adversarial Nets (GANs)](https://arxiv.org/abs/1703.00573)\n",
    "- [Do GANs actually learn the distribution? An empirical study](https://arxiv.org/abs/1706.08224)\n",
    "- Presentation: [Understanding Generative Adversarial Networks](http://www.gatsby.ucl.ac.uk/~balaji/Understanding-GANs.pdf)\n",
    "- [Adversarially Learned Inference](https://ishmaelbelghazi.github.io/ALI/)\n",
    "- [GAN Applications](https://github.com/nashory/gans-awesome-applications)\n",
    "- [From GAN to WGAN](https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html)\n",
    "- [Generative Adversarial Nets in TensorFlow](https://blog.evjang.com/2016/06/generative-adversarial-nets-in.html)\n",
    "- [gan_simple_nb.ipynb](https://github.com/AYLIEN/jupyter-notebooks/blob/master/research/gan/gan_simple_nb.ipynb)\n",
    "- [How to Train a GAN? Tips and tricks to make GANs work](https://github.com/soumith/ganhacks)\n",
    "- [Building a simple Generative Adversarial Network (GAN) using TensorFlow](https://blog.paperspace.com/implementing-gans-in-tensorflow/)\n",
    "- [GAN-1D-distribution-fitting](https://github.com/jankrepl/GAN-1D-distribution-fitting)\n",
    "- An Intuitive Guide to Optimal Transport [part 1](https://www.mindcodec.com/an-intuitive-guide-to-optimal-transport-for-machine-learning/) [part 2](https://www.mindcodec.com/an-intuitive-guide-to-optimal-transport-part-ii-the-wasserstein-gan-made-easy/)\n",
    "- [GANs are Broken in More than One Way: The Numerics of GANs](https://www.inference.vc/my-notes-on-the-numerics-of-gans/)\n",
    "- [GAN — Why it is so hard to train Generative Adversarial Networks!](https://medium.com/@jonathan_hui/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b)\n",
    "- [Mode collapse in GANs](http://aiden.nibali.org/blog/2017-01-18-mode-collapse-gans/)\n",
    "- [Generative Adversarial Networks (GANs) in 50 lines of code (PyTorch)](https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f)\n",
    "- [A Primer on Optimal Transport](https://nips.cc/Conferences/2017/Schedule?showEvent=8736)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch norm\n",
    "- [An Intuitive Explanation of Why Batch Normalization Really Works (Normalization in Deep Learning Part 1)](http://mlexplained.com/2018/01/10/an-intuitive-explanation-of-why-batch-normalization-really-works-normalization-in-deep-learning-part-1/)\n",
    "- [Intuit and Implement: Batch Normalization](https://towardsdatascience.com/intuit-and-implement-batch-normalization-c05480333c5b)\n",
    "- [\n",
    "How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift)](https://arxiv.org/abs/1805.11604)\n",
    "- [Batch normalization in Neural Networks](https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c)\n",
    "- [Batch Normalization — What the hey?\n",
    "](https://gab41.lab41.org/batch-normalization-what-the-hey-d480039a9e3b)\n",
    "- [Understanding the backward pass through Batch Normalization Layer](http://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html)\n",
    "- [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)\n",
    "- [Intro to optimization in deep learning: Busting the myth about batch normalization](https://blog.paperspace.com/busting-the-myths-about-batch-normalization/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc\n",
    "\n",
    "- [Neural Processes as distributions over functions](https://kasparmartens.rbind.io/post/np/)\n",
    "- [Hamiltonian Descent Methods](https://arxiv.org/abs/1809.05042)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
